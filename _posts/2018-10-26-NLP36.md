---
layout: post
title: 6 - 10 занятия курса DL in NLP
---

Краткий конспект по нашим занятиям во второй половине курса.

Записи лекций CS224n:
1. https://youtu.be/Keqep_PKrY8
1. https://youtu.be/QuELiw8tbx8
1. https://youtu.be/IxQtK2SjWWM


Слайды наших лекций:
1. [Seminar 9. Attention is All You Need](https://drive.google.com/file/d/1FfxG2CBLy-7M_nAzg1lP3hJOmgxjz77c/view?usp=sharing)
1. [Seminar 10. 2018 is the Year of Transfer Learning in NLP](https://drive.google.com/file/d/1C5RmioxZCvuMB0XuzXHqgO6MeCiBShu4/view?usp=sharing)


Дополнительные материалы:
1. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
1. [Deep Learning Book](http://www.deeplearningbook.org)
1. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs)
1. [On the difficulty of training recurrent neural networks](http://proceedings.mlr.press/v28/pascanu13.pdf)
1. [Attention](https://www.youtube.com/watch?v=quoGRI-1l0A)
1. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
1. [The Annotated Encoder-Decoder with Attention](https://bastings.github.io/annotated_encoder_decoder)
1. [Good practices in Modern Tensorflow for NLP](https://roamanalytics.com/2018/09/24/good-practices-in-modern-tensorflow-for-nlp/)
1. [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
1. [Illustrated transformer](http://jalammar.github.io/illustrated-transformer/)
1. [A proposal of good practices for files, folders and models architecture](https://blog.metaflow.fr/tensorflow-a-proposal-of-good-practices-for-files-folders-and-models-architecture-f23171501ae3)
1. [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)
1. [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf)
1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
1. [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)
