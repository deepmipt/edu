---
layout: post
title: Четвёртый семинар курса DL in NLP
---

В четверг 25-го октября состоялось четвёртое занятие по курсу Deep Learning in Natural Language Processing.
На семинаре мы обсудили:

1) Три стэнфордских лекции: [cs224n 5](https://youtu.be/isPiE-DBagM),
[cs231n 4](https://youtu.be/i94OvYb6noo),
[cs231n 5](https://youtu.be/gYpoJMlgyXA)

2) Квиз

3) Bаши вопросы к семинару

4) Объявили рекомендуемые финальные проекты

## Заметки по квизу:
1. Производная сигмоиды: sigmoid'(x) = sigmoid(x)(1 - sigmoid(x))
1. Иерархический софтмакс: используется для ускорения (x50) подсчёта вероятности класса.
Для этого мы строим бинарное дерево в листьях которого находятся наши классы. В узлах этого дерева мы используем
sigmoid-активации для получения вероятности пойти "направо" ("налево") по дереву.
Подробнее: [в этом видео](https://www.youtube.com/watch?v=B95LTf2rVWM)
1. Мультитаск-обучение - подход, в котором нейросеть учится решать сразу несколько задач.
Таким образом мы учим более общие представления, которые могут улучшить обобщающую способность.
Например, мы можем одровременно учить классификацию текса и предсказывать следующее слово в тексте.
1. Momentum - модификация алгоритма стохастического градиентного спуска, в котором мы аккумулируем предыдущие градиенты
и учитываем их при шаге оптимизации ([подробнее](https://distill.pub/2017/momentum/)). Его проблемой может быть то, что
если константа, отвечающая за вес предыдущего шага слишком большая, алгоритм может пролететь минимум и начать осцилировать
вокруг него.
1. Random search vs grid search. Ответ и интуиция на картинке.
Подробнее в [5 лекции cs231n c 1:05:55](https://youtu.be/gYpoJMlgyXA?t=3955)
1. Оптимизация гиперпараметров очень важна. Один из самых важных гиперпараметров - learning rate.
1. Мы можем использовать различные learning rate на различных участках сети, потому что learning rate не изменяет направления
градиента, а только изменяет размер шага по этому направлению.

![random_search_vs_grid_search](https://i.stack.imgur.com/cIDuR.png)

## Ответы на ваши вопросы:
1) Как применять backpropagation в случае сети с разрывными функциями активации или узлами с дискретным сигналом?

В случае разрывных функций активации как правило разрыв просто игнорируется
(как, например, в ReLU) - вы реализуете это в третьем домашнем задании.
Узлы с дискретными сигналами - активная область исследований.
Три примера, как это можно делать: [gumbel-softmax](https://arxiv.org/abs/1611.01144),
[policy gradients](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)
(и их [различные модификации](https://arxiv.org/abs/1703.07370)),
[VQ-VAE](https://arxiv.org/abs/1711.00937). Популярно это изложено в
[этом посте](https://medium.com/mini-distill/discrete-optimization-beyond-reinforce-5ca171bebf17) на medium.

2) Используются ли для нейросетей методы оптимизации второго порядка и какие здесь сложности?

Главная проблема методов второго порядка - переход от градиентов, к гессиану и квадратичное увеличение потребления памяти,
что делает их использование на практике зачастую невозможным.
Существуют методы оптимизации, приближающие методы второго порядка методами первого порядка. Например, алгоритм оптимизации
[K-FAC](https://arxiv.org/abs/1503.05671), использующий понятие "натурального градиента".

3) Какие есть известные виды loss-функций и как сложность функции влияет на процесс обучения?

На практике чаще всего используются кросс-энтропия, triplet loss (очень похож на max margin loss, который мы обсуждали в лекции)
и евклидова мера (в задачах регрессии). Новые функции потерь - [активная область исследований](https://arxiv.org/abs/1503.05671).
Как правило, сложность вычисления функции не является проблемой, они все достаточно простые.

## Третье домашнее задание:
В нём вы напишете простую полносвязную нейросеть без фреймворков глубокого обучения - чисто на numpy.
Вам нужно будет реализовать прямой проход и обратный проход.
Код для расчёта нейросети не должен содержать циклов (если не указано обратное), используйте векторизованные операции.
Это может быть не так просто, но почти всегда можно заменить цикл на хитрое умножение матриц и получить ускорение на целый порядок.

Домашнее задание: [simple_nn.ipynb](https://github.com/deepmipt/deep-nlp-seminars/blob/master/seminar_04/simple_nn.ipynb)

__Мягкий дедлайн:__ 2 ноября (23:59)

__Жёсткий дедлайн:__ 11 ноября (23:59)

После мягкого дедлайна максимальный бал за задание будет линейно дисконтироваться
вплоть до половины от максимума в день жёсткого дедлайна.

Чтобы сдать задание, отправьте выполненный ноутбук на почту nlp_course@ipavlov.ai с темой Assignment 3.
Просьба не указывать своё имя в самом ноутбуке.

Очень большая просьба не выкладывать решение домашнего задания в публично доступные репозитории на github / gitlab / etc

## Финальные проекты
Пришло время выбирать проект, над которым вы будете работать до конца курса.

Форма регистрации: [по ссылке](https://goo.gl/forms/Si5CZV98fYsnOieB2)

Рекомендуемые проекты: [по другой ссылке](https://docs.google.com/spreadsheets/d/1XytqPfsCM7tj5RgIFEKU-Cs5QlNF2sREaT_SzcRAvvY/edit?usp=sharing).

Проекты со сложностью "лекго" можно делать только в одиночку и только один человек из всего курса.
Проекты со сложностью "очень сложно" и "сложно" рекомендуем делать в командах до 4 человек и до 3 человек соответственно.
Для проекта по машинному переводу может быть сделано исключение.

В зависимости от числа проектов, к каждому из них будет назначен ментор.
В любом случае будут проведены личные консультации для каждой группы.

__Дедлайн по регистрации проекта:__ середина ноября.

Те, кто зарегистрируется и начнёт делать свои проекты до 10 ноября (включительно),
__будут освобождены от 5 и 6 домашних заданий__.


Оперативнее всего информацию можно получать в нашем телеграм-канале: [https://t.me/dlinnlpfall2018](https://t.me/dlinnlpfall2018)

Если у вас есть вопросы не по семинару, вы можете их обсудить в телеграм-чате: [https://t.me/dlinnlpfall2018discuss](https://t.me/dlinnlpfall2018discuss)
